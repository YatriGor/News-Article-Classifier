# News Article Topic Classifier using Naive Bayes

A machine learning project that classifies BBC news articles into different categories using the Naive Bayes algorithm implemented from scratch in Python.

## Table of Contents

- [Overview](#overview)
- [Naive Bayes Algorithm Explained](#naive-bayes-algorithm-explained)
- [Example Walkthrough](#example-walkthrough)
- [Code Implementation](#code-implementation)
- [Dataset](#dataset)
- [Features](#features)
- [Results and Evaluation](#results-and-evaluation)
- [Requirements](#requirements)
- [Usage](#usage)
- [üìö Detailed Code Walkthrough](#detailed-code-walkthrough)

## Overview

This project implements a **Naive Bayes classifier** from scratch to categorize BBC news articles into topics like Sports, Politics, Business, Entertainment, and Technology. The implementation includes text preprocessing, model training, prediction, and comprehensive evaluation metrics with visualizations.

## Naive Bayes Algorithm Explained

### What is Naive Bayes?

Naive Bayes is a probabilistic classification algorithm based on **Bayes' Theorem** with a "naive" assumption that all features (words in our case) are independent of each other.

### Bayes' Theorem

```
P(Class|Document) = P(Document|Class) √ó P(Class) / P(Document)
```

Where:

- **P(Class|Document)**: Probability of a document belonging to a class (what we want to find)
- **P(Document|Class)**: Probability of seeing this document given the class
- **P(Class)**: Prior probability of the class
- **P(Document)**: Probability of the document (constant for all classes)

### The "Naive" Assumption

We assume that each word in a document is independent of every other word. So:

```
P(Document|Class) = P(word1|Class) √ó P(word2|Class) √ó ... √ó P(wordN|Class)
```

### Why Use Log Probabilities?

Since we multiply many small probabilities, we use logarithms to prevent numerical underflow:

```
log(P(Class|Document)) = log(P(Class)) + Œ£ log(P(word|Class))
```

## Example Walkthrough

Let's say we want to classify this article: **"The football team won the championship"**

### Step 1: Training Data

Imagine we have training data:

- **Sports**: "football team", "basketball game", "championship match"
- **Politics**: "government policy", "election results", "political party"

### Step 2: Calculate Prior Probabilities

```
P(Sports) = Number of Sports articles / Total articles = 1/2 = 0.5
P(Politics) = Number of Politics articles / Total articles = 1/2 = 0.5
```

### Step 3: Calculate Word Likelihoods

For the word "football":

```
P("football"|Sports) = (Count of "football" in Sports + Œ±) / (Total words in Sports + Œ± √ó Vocabulary size)
P("football"|Politics) = (Count of "football" in Politics + Œ±) / (Total words in Politics + Œ± √ó Vocabulary size)
```

Where Œ± (alpha) is the **Laplace smoothing** parameter to handle unseen words.

### Step 4: Make Prediction

For our test sentence "The football team won the championship":

```
Score(Sports) = log(P(Sports)) + log(P("football"|Sports)) + log(P("team"|Sports)) + log(P("won"|Sports)) + log(P("championship"|Sports))
Score(Politics) = log(P(Politics)) + log(P("football"|Politics)) + log(P("team"|Politics)) + log(P("won"|Politics)) + log(P("championship"|Politics))
```

The class with the higher score wins!

## Code Implementation

### 1. Data Loading and Splitting (`load_data`, `split_data`)

```python
# Loads BBC News CSV file
all_articles, all_labels = load_data('BBC News Train.csv')

# Splits data into 80% training and 20% validation
train_articles, train_labels, val_articles, val_labels = split_data(all_articles, all_labels)
```

**What it does**: Reads the CSV file containing news articles and their categories, then randomly splits the data for training and testing.

### 2. Text Preprocessing (`preprocess_text`)

```python
def preprocess_text(text):
    # 1. Convert to lowercase
    # 2. Remove punctuation
    # 3. Remove numbers
    # 4. Split into words (tokenization)
    # 5. Remove stop words (common words like "the", "and", "is")
    return cleaned_tokens
```

**Example**:

- Input: "The U.S. government announced new policies!"
- Output: ['government', 'announced', 'new', 'policies']

**Why preprocessing?**: Raw text contains noise. Preprocessing standardizes text and focuses on meaningful words.

### 3. Naive Bayes Implementation (`MyNaiveBayes`)

#### Training Phase (`fit` method):

1. **Build Vocabulary**: Collect all unique words from training data
2. **Calculate Prior Probabilities**: `P(Class) = Documents in Class / Total Documents`
3. **Calculate Word Likelihoods**: For each word and class combination using Laplace smoothing

#### Prediction Phase (`predict` method):

1. For each test document, calculate log probabilities for each class
2. Return the class with the highest probability

### 4. Laplace Smoothing

```python
likelihood = (word_count + Œ±) / (total_words_in_class + Œ± √ó vocabulary_size)
```

**Why needed?**: If a word appears in the test data but not in the training data for a specific class, the probability would be 0, making the entire prediction 0. Laplace smoothing adds a small value (Œ±=1) to avoid this.

### 5. Evaluation Metrics

#### Confusion Matrix

Shows how many articles were correctly/incorrectly classified for each category:

```
                Predicted
Actual    Sport  Politics  Business
Sport       85      2         3
Politics     1     78         1
Business     2      1        87
```

#### Precision and Recall

- **Precision**: Of all articles predicted as Sports, how many were actually Sports?
- **Recall**: Of all actual Sports articles, how many were correctly identified?

#### Visualization

The code generates bar charts showing the top 10 most probable words for each news category, helping understand what the model learned.

## Dataset

The project uses the **BBC News Dataset** with columns:

- **ArticleId**: Unique identifier
- **Article**: News article text
- **Category**: Topic category (Sport, Politics, Business, Entertainment, Tech)

## Features

### Core Features:

- ‚úÖ **From-scratch implementation** of Naive Bayes
- ‚úÖ **Text preprocessing pipeline** (lowercasing, punctuation removal, stop words filtering)
- ‚úÖ **Laplace smoothing** for handling unseen words
- ‚úÖ **Log probabilities** to prevent numerical underflow
- ‚úÖ **Comprehensive evaluation** (accuracy, precision, recall, confusion matrix)
- ‚úÖ **Data visualization** of top words per category

### Advanced Features:

- üìä **Interactive plots** showing most characteristic words per category
- üîÄ **Random data shuffling** for unbiased train/test split
- üìà **Detailed performance metrics** for each category
- üéØ **Example predictions** with actual vs predicted labels

## Results and Evaluation

The model provides:

1. **Overall accuracy percentage** on validation data
2. **Confusion matrix** showing classification performance per category
3. **Precision and Recall** for each news category
4. **Visual analysis** of top discriminative words
5. **Sample predictions** with explanations

## Requirements

```
matplotlib
```

No external ML libraries required - pure Python implementation!

## Usage

1. **Ensure your BBC News CSV file is in the project directory**
2. **Run the script**:
   ```bash
   python project.py
   ```
3. **View results**:
   - Console output shows training progress and metrics
   - Plot window displays top words per category
   - Close the plot window to finish execution

## How the Algorithm Works in This Scenario

### Real Example from the Code:

1. **Training**: The model learns that words like "goal", "match", "player" frequently appear in Sports articles
2. **Vocabulary Building**: Creates a set of all unique words across all articles
3. **Probability Calculation**: For each category, calculates how likely each word is to appear
4. **Classification**: When given a new article about "football championship", it calculates:
   - How likely is this article to be Sports? (high probability for "football", "championship")
   - How likely is this article to be Politics? (low probability for these words)
   - Chooses the category with highest probability

### Key Strengths:

- ‚úÖ **Fast training and prediction**
- ‚úÖ **Works well with limited data**
- ‚úÖ **Interpretable results** (can see which words influence decisions)
- ‚úÖ **Handles new words** gracefully with smoothing

### Limitations:

- ‚ùå **Independence assumption** (words aren't truly independent)
- ‚ùå **Doesn't capture word order** ("not good" vs "good not")
- ‚ùå **Sensitive to skewed data** (some categories have more examples)

This implementation demonstrates fundamental NLP and machine learning concepts while providing a complete, working text classifier!

## üìö Detailed Code Walkthrough

For a **complete step-by-step breakdown** of every function with mathematical formulas and practical examples, see:

**[üìñ CODE_WALKTHROUGH.md](CODE_WALKTHROUGH.md)**

This detailed guide includes:

- üßÆ **Exact mathematical formulas** used in each function
- üìù **Line-by-line code explanations** with examples
- üìä **Numerical examples** showing calculations
- üîç **Algorithm insights** and computational complexity
- üí° **Why each step is necessary** for the classification process

Perfect for understanding the implementation details and mathematical foundations!
